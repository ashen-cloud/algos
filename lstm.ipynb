{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBCCDXwX3DHm2/5sGMVvIh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHn6K0u3-2lF",
        "outputId": "69d76099-049d-40d6-9308-d0a50f425e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab\n",
        "\n",
        "\n",
        "from functools import partial,reduce\n",
        "from tqdm import tqdm, trange\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "trange = partial(trange, position=0, leave=True)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "ul = url.split('/')\n",
        "name = ul[len(ul) - 1]\n",
        "\n",
        "with open(name, 'wb') as file:\n",
        "  file.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(name, \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./\")\n",
        "\n",
        "!mv 'cornell movie-dialogs corpus' 'data'\n",
        "!ls 'data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN_cyVT6eQQM",
        "outputId": "907bb639-717a-489f-f1ff-a299f0c7e510"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " chameleons.pdf\t\t\t movie_lines.txt\n",
            "'cornell movie-dialogs corpus'\t movie_titles_metadata.txt\n",
            " movie_characters_metadata.txt\t raw_script_urls.txt\n",
            " movie_conversations.txt\t README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V_jfwwVQKw-o"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIELD_SPLITTER = '+++$+++'\n",
        "\n",
        "MAX_SAMPLES = 50000\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "UNK_TOKEN = '<unk>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "UNK_TOKEN_IND = 0\n",
        "PAD_TOKEN_IND = 1\n",
        "BOS_TOKEN_IND = 2\n",
        "EOS_TOKEN_IND = 3\n",
        "\n",
        "BATCH = 128"
      ],
      "metadata": {
        "id": "Kx8U5FhayR-T"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PW9kkcKGvkht"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-FmVEHllv777"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = lambda x, voc, tokenizer: [voc['<BOS>']] + [voc[token] for token in tokenizer(x)] + [voc['<EOS>']]"
      ],
      "metadata": {
        "id": "5xjiwfyZwZbt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is terrible as fuck because torchtext is terrible as fuck\n",
        "def load_conversations(path_to_movie_lines, path_to_movie_conversations):\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, errors='ignore') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(path_to_movie_conversations, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "            for i in range(len(conversation) - 1):\n",
        "                inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "                outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "                if len(inputs) >= MAX_SAMPLES:\n",
        "                    return inputs, outputs\n",
        "    return inputs, outputs\n"
      ],
      "metadata": {
        "id": "-sijVHLArNQJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(path_to_movie_lines,\n",
        "                   path_to_movie_conversations):\n",
        "    questions, answers = load_conversations(path_to_movie_lines, path_to_movie_conversations)\n",
        "\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    counter = Counter()\n",
        "    for sent in questions + answers:\n",
        "        counter.update(tokenizer(sent))\n",
        "\n",
        "    voc = vocab(counter)\n",
        "    voc.insert_token(token=UNK_TOKEN, index=UNK_TOKEN_IND)\n",
        "    voc.set_default_index(index=UNK_TOKEN_IND)\n",
        "    voc.insert_token(token=PAD_TOKEN, index=PAD_TOKEN_IND)\n",
        "    voc.insert_token(token=BOS_TOKEN, index=BOS_TOKEN_IND)\n",
        "    voc.insert_token(token=EOS_TOKEN, index=EOS_TOKEN_IND)\n",
        "\n",
        "    q_tokenized = [text_transform(t, voc, tokenizer) for t in questions]\n",
        "    a_tokenized = [text_transform(t, voc, tokenizer) for t in answers]\n",
        "\n",
        "    import tensorflow as tf # todo\n",
        "    q_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        q_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    a_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        a_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    print(\"Vocab len\", len(voc))\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        list(\n",
        "            zip(\n",
        "                  q_padded.astype(np.float32),\n",
        "                  a_padded.astype(np.float32),\n",
        "                )\n",
        "            ),\n",
        "            batch_size=BATCH,\n",
        "            shuffle=False,\n",
        "    )\n",
        "\n",
        "    print(voc)\n",
        "    torch.save(voc, 'vocab')\n",
        "\n",
        "    return dataloader, text_transform, voc\n"
      ],
      "metadata": {
        "id": "dblp_TVLqvav"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3T8xN1yxmyc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_path = 'data/movie_lines.txt'\n",
        "conversations_path = 'data/movie_conversations.txt'    \n",
        "\n",
        "dataloader, text_transform, voc = get_dataloader(lines_path,\n",
        "                                                 conversations_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42tdLcayovn",
        "outputId": "b3de1222-71b6-4da2-d17b-75389cb24458"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab len 23068\n",
            "Vocab()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = None\n",
        "\n",
        "for i,x in enumerate(dataloader):\n",
        "  if i > 1: break\n",
        "  print(x[0].shape)\n",
        "  test_sample = x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMyWFI7N3VVf",
        "outputId": "1461aec8-bb06-4478-d9be-5b9b5f8f8166"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 40])\n",
            "torch.Size([128, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OG_PUCipfDAT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fneural-networks-and%2F9781492037354%2Fassets%2Fmlst_1413.png&f=1&nofb=1\n",
        "class myLSTM(nn.Module):\n",
        "  def __init__(self, isize, osize):\n",
        "    super(myLSTM, self).__init__()\n",
        "\n",
        "    self.isize = isize\n",
        "    self.osize = osize\n",
        "\n",
        "    # sizes are questionable\n",
        "    self.f = nn.Linear(isize, osize)\n",
        "    self.g = nn.Linear(isize, osize)\n",
        "    self.i = nn.Linear(isize, osize)\n",
        "    self.o = nn.Linear(isize, osize)\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forget_gate(self, h, x, c):\n",
        "    fho = self.f(h)\n",
        "    fxo = self.f(x)\n",
        "\n",
        "    fo = fho + fxo\n",
        "    fos = self.sigmoid(fo)\n",
        "    out = torch.mul(c, fos)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def input_gate(self, h, x):\n",
        "    gho = self.g(h)\n",
        "    gxo = self.g(x)\n",
        "    go = gho + gxo\n",
        "    gto = self.tanh(go)\n",
        "\n",
        "    iho = self.i(h)\n",
        "    ixo = self.i(x)\n",
        "    io = iho + ixo\n",
        "    ito = self.sigmoid(io)\n",
        "\n",
        "    out = torch.mul(gto, ito)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def output_gate(self, h, x, cn):\n",
        "    oho = self.o(h)\n",
        "    oxo = self.o(x)\n",
        "    oo = oho + oxo\n",
        "\n",
        "    out = torch.mul(oo, cn)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def forward(self, x, h0=None, c0=None):\n",
        "    if not h0:\n",
        "      h0 = torch.zeros(x.shape[0], self.isize)\n",
        "    if not c0:\n",
        "      c0 = torch.zeros(x.shape[0], self.osize)\n",
        "\n",
        "    fg_out = self.forget_gate(h0, x, c0)\n",
        "    ig_out = self.input_gate(h0, x)\n",
        "\n",
        "    c_new = fg_out + ig_out\n",
        "\n",
        "    cnt = self.tanh(c_new) \n",
        "\n",
        "    h_new = self.output_gate(h0, x, c_new)\n",
        "\n",
        "    out = h_new\n",
        "\n",
        "    # is this for real how it's done?\n",
        "    return out, (h_new[len(h_new) - 1].reshape(1, self.osize),\n",
        "                 c_new[len(h_new) - 1].reshape(1, self.osize))\n"
      ],
      "metadata": {
        "id": "Iajk3iU8hKlB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lstm = myLSTM(40, 80)\n",
        "\n",
        "for p in my_lstm.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "out, (h, c) = my_lstm(test_sample)\n",
        "\n",
        "print(out.shape)\n",
        "print('\\n----------------')\n",
        "print(h.shape)\n",
        "print('\\n----------------')\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic6e2QJcHlqj",
        "outputId": "86991be2-125c-4d4d-c744-d0ee864f7086"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "\n",
            "\n",
            "torch.Size([128, 80])\n",
            "\n",
            "----------------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "----------------\n",
            "torch.Size([1, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YImpb8ymHlsy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcvxNbhgfh3S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sample.shape)\n",
        "print(test_sample.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_FMbHcG3-e-",
        "outputId": "513b6089-2726-4c5d-d4ed-50ecdee194ff"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 40])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_lstm = nn.LSTM(40, 80, batch_first=True)\n",
        "\n",
        "for p in t_lstm.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "res, (h_s, c_s) = t_lstm(test_sample)\n",
        "\n",
        "print(res.shape)\n",
        "print('\\n---------')\n",
        "print(h_s.shape)\n",
        "print('\\n---------')\n",
        "print(c_s.shape)\n",
        "print('\\n---------')"
      ],
      "metadata": {
        "id": "QfkMe8hL0oi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5befa565-bc63-4dda-a88d-3a5ccdcad65e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([320, 40])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320])\n",
            "torch.Size([320])\n",
            "\n",
            "\n",
            "torch.Size([128, 80])\n",
            "\n",
            "---------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "---------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jxr4MUuSKxHR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.lstm = nn.LSTM(40, 80, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, (h, c) = self.lstm(x)\n",
        "    return x, (h, c)"
      ],
      "metadata": {
        "id": "B_OfGA35gbyV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.lstm = nn.LSTM(80, 80, batch_first=True)\n",
        "      self.lin = nn.Linear(80, 40)\n",
        "\n",
        "  def forward(self, x):\n",
        "    e_o, (h, c) = x\n",
        "    o, _ = self.lstm(e_o, (h, c))\n",
        "    o = self.lin(o)\n",
        "    return x"
      ],
      "metadata": {
        "id": "daNZmFaLuKfv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WZf8kFUluKhx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wi7GRvfpevfS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().to(DEVICE)\n",
        "decoder = Decoder().to(DEVICE)\n",
        "\n",
        "e_optim = optim.Adam(params=encoder.parameters(), lr=0.001)\n",
        "d_optim = optim.Adam(params=decoder.parameters(), lr=0.001)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "EPOCHS = 4\n",
        "\n",
        "losses = []"
      ],
      "metadata": {
        "id": "lKZsejM8kabv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in trange(EPOCHS):\n",
        "  for i, data in tqdm(enumerate(dataloader)):\n",
        "    i_seq = data[0].to(DEVICE)\n",
        "\n",
        "    e_out = encoder(i_seq)\n",
        "    d_out, _ = decoder(e_out)\n",
        "\n",
        "    loss = loss_fn(e_out[0], d_out)\n",
        "\n",
        "    e_optim.zero_grad()\n",
        "    d_optim.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    e_optim.step()\n",
        "    d_optim.step()"
      ],
      "metadata": {
        "id": "QtVP3qoOKxRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c945a2a-5e65-454f-aa8a-f599a28b01c7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "391it [00:02, 174.12it/s]\n",
            "391it [00:02, 177.88it/s]\n",
            "391it [00:02, 177.87it/s]\n",
            "391it [00:02, 179.40it/s]\n",
            "100%|██████████| 4/4 [00:08<00:00,  2.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(losses)), losses, 'b', label='Loss')\n",
        "plt.title('Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y4GSVA0xKxVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c7b5e93a-1d3a-4641-fe91-61742197cee1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'test'\n",
        "\n",
        "q_tr = text_transform(query, voc, get_tokenizer('basic_english'))\n",
        "import tensorflow as tf\n",
        "q_p = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    [q_tr],\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding='post',\n",
        "    value=1.0\n",
        ")\n",
        "q_t = torch.Tensor(q_p).int().to(DEVICE)\n",
        "\n",
        "\n",
        "enc, _ = encoder(q_t)\n",
        "dec = decoder(enc)\n",
        "\n",
        "int_infer = dec.int()\n",
        "\n",
        "tokens = [t.item() for t in int_infer[0]]\n",
        "words = voc.get_itos()\n",
        "result = ' '.join(list(filter(lambda w: '<' not in w and '>' not in w, [words[t] for t in tokens])))\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "s1cpwVbSKxZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "7a3aff75-53d0-4647-b2fa-fed6abc5ac28"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-d11fb69c4eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d60fbe62babe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 762\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_cuda\" not implemented for 'Int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Up2jkK36zGAD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xrr5i0IpzGCn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LGWG3mbzzGEx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mum903TXzGHY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LfUhnHL_Kxcj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ddwvBDsnKxf0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 10 data/movie_conversations.txt "
      ],
      "metadata": {
        "id": "4kVqD08vKxjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54509d0-c48e-4f55-b5eb-74b62f0ddf66"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "u9027 +++$+++ u9029 +++$+++ m616 +++$+++ ['L666460', 'L666461']\n",
            "u9027 +++$+++ u9029 +++$+++ m616 +++$+++ ['L666485', 'L666486']\n",
            "u9027 +++$+++ u9029 +++$+++ m616 +++$+++ ['L666546', 'L666547']\n",
            "u9028 +++$+++ u9033 +++$+++ m616 +++$+++ ['L666497', 'L666498', 'L666499', 'L666500', 'L666501', 'L666502']\n",
            "u9028 +++$+++ u9031 +++$+++ m616 +++$+++ ['L666262', 'L666263', 'L666264']\n",
            "u9028 +++$+++ u9031 +++$+++ m616 +++$+++ ['L666324', 'L666325', 'L666326', 'L666327']\n",
            "u9028 +++$+++ u9031 +++$+++ m616 +++$+++ ['L666575', 'L666576']\n",
            "u9030 +++$+++ u9034 +++$+++ m616 +++$+++ ['L666256', 'L666257']\n",
            "u9030 +++$+++ u9034 +++$+++ m616 +++$+++ ['L666369', 'L666370', 'L666371', 'L666372']\n",
            "u9030 +++$+++ u9034 +++$+++ m616 +++$+++ ['L666520', 'L666521', 'L666522']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "zKWhBqLmKxoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a4cbb4-7205-4a07-d618-404bc9a2115f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " chameleons.pdf\t\t\t movie_lines.txt\n",
            "'cornell movie-dialogs corpus'\t movie_titles_metadata.txt\n",
            " movie_characters_metadata.txt\t raw_script_urls.txt\n",
            " movie_conversations.txt\t README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/README.txt"
      ],
      "metadata": {
        "id": "mm4in0NdubQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74a68dd-b691-4716-ed11-3135205f2a17"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cornell Movie-Dialogs Corpus\n",
            "\n",
            "Distributed together with:\n",
            "\n",
            "\"Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs\"\n",
            "Cristian Danescu-Niculescu-Mizil and Lillian Lee\n",
            "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011.\r\n",
            "\n",
            "(this paper is included in this zip file)\n",
            "\n",
            "NOTE: If you have results to report on these corpora, please send email to cristian@cs.cornell.edu or llee@cs.cornell.edu so we can add you to our list of people using this data.  Thanks!\n",
            "\n",
            "\n",
            "Contents of this README:\n",
            "\n",
            "\tA) Brief description\n",
            "\tB) Files description\n",
            "\tC) Details on the collection procedure\n",
            "\tD) Contact\n",
            "\n",
            "\n",
            "A) Brief description:\n",
            "\n",
            "This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts:\n",
            "\n",
            "- 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
            "- involves 9,035 characters from 617 movies\n",
            "- in total 304,713 utterances\n",
            "- movie metadata included:\n",
            "\t- genres\n",
            "\t- release year\n",
            "\t- IMDB rating\n",
            "\t- number of IMDB votes\n",
            "\t- IMDB rating\n",
            "- character metadata included:\n",
            "\t- gender (for 3,774 characters)\n",
            "\t- position on movie credits (3,321 characters)\n",
            "\n",
            "\n",
            "B) Files description:\n",
            "\n",
            "In all files the field separator is \" +++$+++ \"\n",
            "\n",
            "- movie_titles_metadata.txt\n",
            "\t- contains information about each movie title\n",
            "\t- fields: \n",
            "\t\t- movieID, \n",
            "\t\t- movie title,\n",
            "\t\t- movie year, \n",
            "\t   \t- IMDB rating,\n",
            "\t\t- no. IMDB votes,\n",
            " \t\t- genres in the format ['genre1','genre2',�,'genreN']\n",
            "\n",
            "- movie_characters_metadata.txt\n",
            "\t- contains information about each movie character\n",
            "\t- fields:\n",
            "\t\t- characterID\n",
            "\t\t- character name\n",
            "\t\t- movieID\n",
            "\t\t- movie title\n",
            "\t\t- gender (\"?\" for unlabeled cases)\n",
            "\t\t- position in credits (\"?\" for unlabeled cases) \n",
            "\n",
            "- movie_lines.txt\n",
            "\t- contains the actual text of each utterance\n",
            "\t- fields:\n",
            "\t\t- lineID\n",
            "\t\t- characterID (who uttered this phrase)\n",
            "\t\t- movieID\n",
            "\t\t- character name\n",
            "\t\t- text of the utterance\n",
            "\n",
            "- movie_conversations.txt\n",
            "\t- the structure of the conversations\n",
            "\t- fields\n",
            "\t\t- characterID of the first character involved in the conversation\n",
            "\t\t- characterID of the second character involved in the conversation\n",
            "\t\t- movieID of the movie in which the conversation occurred\n",
            "\t\t- list of the utterances that make the conversation, in chronological \n",
            "\t\t\torder: ['lineID1','lineID2',�,'lineIDN']\n",
            "\t\t\thas to be matched with movie_lines.txt to reconstruct the actual content\n",
            "\n",
            "- raw_script_urls.txt\n",
            "\t- the urls from which the raw sources were retrieved\n",
            "\n",
            "C) Details on the collection procedure:\n",
            "\n",
            "We started from raw publicly available movie scripts (sources acknowledged in \n",
            "raw_script_urls.txt).  In order to collect the metadata necessary for this study \n",
            "and to distinguish between two script versions of the same movie, we automatically\n",
            " matched each script with an entry in movie database provided by IMDB (The Internet\n",
            " Movie Database; data interfaces available at http://www.imdb.com/interfaces). Some\n",
            " amount of manual correction was also involved. When  more than one movie with the same\n",
            " title was found in IMBD, the match was made with the most popular title \n",
            "(the one that received most IMDB votes)  \n",
            "\n",
            "After discarding all movies that could not be matched or that had less than 5 IMDB \n",
            "votes, we were left with 617 unique titles with metadata including genre, release \n",
            "year, IMDB rating and no. of IMDB votes and cast distribution.  We then identified \n",
            "the pairs of characters that interact and separated their conversations automatically \n",
            "using simple data processing heuristics. After discarding all pairs that exchanged \n",
            "less than 5 conversational exchanges there were 10,292 left, exchanging 220,579 \n",
            "conversational exchanges (304,713 utterances).  After automatically matching the names \n",
            "of the 9,035 involved characters to the list of cast distribution, we used the \n",
            "gender of each interpreting actor to infer the fictional gender of a subset of \n",
            "3,321 movie characters (we raised the number of gendered 3,774 characters through\n",
            " manual annotation). Similarly, we collected the end credit position of a subset \n",
            "of 3,321 characters as a proxy for their status.\n",
            "\n",
            "\n",
            "D) Contact:\n",
            "\n",
            "Please email any questions to: cristian@cs.cornell.edu (Cristian Danescu-Niculescu-Mizil)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # trash\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def preprocess(x):\n",
        "#   x_no_new = x.replace('\\n', '')\n",
        "#   text = x_no_new.split(FIELD_SPLITTER).pop()\n",
        "#   embedding = g_vectors.get_vecs_by_tokens(tokenizer(text), lower_case_backup=True)\n",
        "#   return embedding\n",
        "\n",
        "# tokenizer = get_tokenizer('basic_english')\n",
        "# g_vectors = GloVe(name='840B')\n",
        "# g_vocab = vocab(g_vectors.stoi)\n",
        "\n",
        "\n",
        "# train_iter = tt.data.BucketIterator(\n",
        "#   dataset=train_obj,\n",
        "#   batch_size = 2,\n",
        "#   sort_key=lambda x: len(x.review),\n",
        "#   shuffle=True,\n",
        "#   device=DEVICE\n",
        "# )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(\n",
        "# \t,\n",
        "# \tbatch_size=BATCH,\n",
        "# \tnum_workers=12,\n",
        "# \tshuffle=True\n",
        "# )"
      ],
      "metadata": {
        "id": "UBDoHXDJsqu4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"),\n",
        "#                                                lower_case_backup=True)\n",
        "# embeddings\n",
        "# \n",
        "# \n",
        "# \n",
        "# def batch(iterable, size):\n",
        "#     from itertools import chain, islice\n",
        "#     iterator = iter(iterable)\n",
        "#     for first in iterator:\n",
        "#         yield list(chain([first], islice(iterator, size - 1)))"
      ],
      "metadata": {
        "id": "DRxfxpjQsrTJ"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}