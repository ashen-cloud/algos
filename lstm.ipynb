{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJtQg5PhOP6aD1mtdLTtmb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHn6K0u3-2lF",
        "outputId": "ed5541f8-4c95-4d6a-b4b2-05595ccc898c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab\n",
        "\n",
        "\n",
        "from functools import partial,reduce\n",
        "from tqdm import tqdm, trange\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "trange = partial(trange, position=0, leave=True)\n",
        "\n",
        "\n",
        "DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "ul = url.split('/')\n",
        "name = ul[len(ul) - 1]\n",
        "\n",
        "with open(name, 'wb') as file:\n",
        "  file.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(name, \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./\")\n",
        "\n",
        "!mv 'cornell movie-dialogs corpus' 'data'\n",
        "!ls 'data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN_cyVT6eQQM",
        "outputId": "f863d908-b851-4641-ed87-d01b960f3500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " chameleons.pdf\t\t\t movie_lines.txt\n",
            "'cornell movie-dialogs corpus'\t movie_titles_metadata.txt\n",
            " movie_characters_metadata.txt\t raw_script_urls.txt\n",
            " movie_conversations.txt\t README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V_jfwwVQKw-o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIELD_SPLITTER = '+++$+++'\n",
        "\n",
        "MAX_SAMPLES = 50000\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "UNK_TOKEN = '<unk>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "UNK_TOKEN_IND = 0\n",
        "PAD_TOKEN_IND = 1\n",
        "BOS_TOKEN_IND = 2\n",
        "EOS_TOKEN_IND = 3\n",
        "\n",
        "BATCH = 4"
      ],
      "metadata": {
        "id": "Kx8U5FhayR-T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PW9kkcKGvkht"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-FmVEHllv777"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = lambda x, voc, tokenizer: [voc['<BOS>']] + [voc[token] for token in tokenizer(x)] + [voc['<EOS>']]"
      ],
      "metadata": {
        "id": "5xjiwfyZwZbt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is terrible as fuck because torchtext is terrible as fuck\n",
        "def load_conversations(path_to_movie_lines, path_to_movie_conversations):\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, errors='ignore') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(path_to_movie_conversations, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "            for i in range(len(conversation) - 1):\n",
        "                inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "                outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "                if len(inputs) >= MAX_SAMPLES:\n",
        "                    return inputs, outputs\n",
        "    return inputs, outputs\n"
      ],
      "metadata": {
        "id": "-sijVHLArNQJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(path_to_movie_lines,\n",
        "                   path_to_movie_conversations):\n",
        "    questions, answers = load_conversations(path_to_movie_lines, path_to_movie_conversations)\n",
        "\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    counter = Counter()\n",
        "    for sent in questions + answers:\n",
        "        counter.update(tokenizer(sent))\n",
        "\n",
        "    voc = vocab(counter)\n",
        "    voc.insert_token(token=UNK_TOKEN, index=UNK_TOKEN_IND)\n",
        "    voc.set_default_index(index=UNK_TOKEN_IND)\n",
        "    voc.insert_token(token=PAD_TOKEN, index=PAD_TOKEN_IND)\n",
        "    voc.insert_token(token=BOS_TOKEN, index=BOS_TOKEN_IND)\n",
        "    voc.insert_token(token=EOS_TOKEN, index=EOS_TOKEN_IND)\n",
        "\n",
        "    q_tokenized = [text_transform(t, voc, tokenizer) for t in questions]\n",
        "    a_tokenized = [text_transform(t, voc, tokenizer) for t in answers]\n",
        "\n",
        "    import tensorflow as tf # todo\n",
        "    q_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        q_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    a_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        a_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    print(\"Vocab len\", len(voc))\n",
        "\n",
        "    dataloader = DataLoader(list(zip(q_padded, a_padded)), batch_size=BATCH, shuffle=False)\n",
        "\n",
        "    print(voc)\n",
        "    torch.save(voc, 'vocab')\n",
        "\n",
        "    return dataloader, text_transform, voc\n"
      ],
      "metadata": {
        "id": "dblp_TVLqvav"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3T8xN1yxmyc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_path = 'data/movie_lines.txt'\n",
        "conversations_path = 'data/movie_conversations.txt'    \n",
        "\n",
        "dataloader, text_transform, voc = get_dataloader(lines_path,\n",
        "                                                 conversations_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42tdLcayovn",
        "outputId": "840a2cc8-ae69-46b1-d394-bc9da278fbd7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab len 23068\n",
            "Vocab()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in enumerate(dataloader):\n",
        "  if i > 1: break\n",
        "  print(x[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMyWFI7N3VVf",
        "outputId": "10590c97-5f3e-4af8-b4bd-ee7d93766029"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 40])\n",
            "torch.Size([4, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OG_PUCipfDAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Iajk3iU8hKlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcvxNbhgfh3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QfkMe8hL0oi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jxr4MUuSKxHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B_OfGA35gbyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wi7GRvfpevfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QtVP3qoOKxRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y4GSVA0xKxVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s1cpwVbSKxZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Up2jkK36zGAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xrr5i0IpzGCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LGWG3mbzzGEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mum903TXzGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LfUhnHL_Kxcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ddwvBDsnKxf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/movie_conversations.txt | tail -n 10"
      ],
      "metadata": {
        "id": "4kVqD08vKxjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "zKWhBqLmKxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/README.txt"
      ],
      "metadata": {
        "id": "mm4in0NdubQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # trash\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def preprocess(x):\n",
        "#   x_no_new = x.replace('\\n', '')\n",
        "#   text = x_no_new.split(FIELD_SPLITTER).pop()\n",
        "#   embedding = g_vectors.get_vecs_by_tokens(tokenizer(text), lower_case_backup=True)\n",
        "#   return embedding\n",
        "\n",
        "# tokenizer = get_tokenizer('basic_english')\n",
        "# g_vectors = GloVe(name='840B')\n",
        "# g_vocab = vocab(g_vectors.stoi)\n",
        "\n",
        "\n",
        "# train_iter = tt.data.BucketIterator(\n",
        "#   dataset=train_obj,\n",
        "#   batch_size = 2,\n",
        "#   sort_key=lambda x: len(x.review),\n",
        "#   shuffle=True,\n",
        "#   device=DEVICE\n",
        "# )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(\n",
        "# \t,\n",
        "# \tbatch_size=BATCH,\n",
        "# \tnum_workers=12,\n",
        "# \tshuffle=True\n",
        "# )"
      ],
      "metadata": {
        "id": "UBDoHXDJsqu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"),\n",
        "#                                                lower_case_backup=True)\n",
        "# embeddings\n",
        "# \n",
        "# \n",
        "# \n",
        "# def batch(iterable, size):\n",
        "#     from itertools import chain, islice\n",
        "#     iterator = iter(iterable)\n",
        "#     for first in iterator:\n",
        "#         yield list(chain([first], islice(iterator, size - 1)))"
      ],
      "metadata": {
        "id": "DRxfxpjQsrTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}