{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHy+lFlCeFWXDvnTSzzhvv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHn6K0u3-2lF",
        "outputId": "4f247530-0284-4830-dec5-23a2cf5d9ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe, vocab\n",
        "\n",
        "\n",
        "from functools import partial,reduce\n",
        "from tqdm import tqdm, trange\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "trange = partial(trange, position=0, leave=True)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "ul = url.split('/')\n",
        "name = ul[len(ul) - 1]\n",
        "\n",
        "with open(name, 'wb') as file:\n",
        "  file.write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(name, \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"./\")\n",
        "\n",
        "!mv 'cornell movie-dialogs corpus' 'data'\n",
        "!ls 'data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN_cyVT6eQQM",
        "outputId": "68c739ad-23df-4601-fb70-1ce754af821a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot move 'cornell movie-dialogs corpus' to 'data/cornell movie-dialogs corpus': Directory not empty\n",
            " chameleons.pdf\t\t\t movie_lines.txt\n",
            "'cornell movie-dialogs corpus'\t movie_titles_metadata.txt\n",
            " movie_characters_metadata.txt\t raw_script_urls.txt\n",
            " movie_conversations.txt\t README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V_jfwwVQKw-o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FIELD_SPLITTER = '+++$+++'\n",
        "\n",
        "MAX_SAMPLES = 50000\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "UNK_TOKEN = '<unk>'\n",
        "PAD_TOKEN = '<PAD>'\n",
        "BOS_TOKEN = '<BOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "UNK_TOKEN_IND = 0\n",
        "PAD_TOKEN_IND = 1\n",
        "BOS_TOKEN_IND = 2\n",
        "EOS_TOKEN_IND = 3\n",
        "\n",
        "BATCH = 3"
      ],
      "metadata": {
        "id": "Kx8U5FhayR-T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PW9kkcKGvkht"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-FmVEHllv777"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transform = lambda x, voc, tokenizer: [voc['<BOS>']] + [voc[token] for token in tokenizer(x)] + [voc['<EOS>']]"
      ],
      "metadata": {
        "id": "5xjiwfyZwZbt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is terrible as fuck because torchtext is terrible as fuck\n",
        "def load_conversations(path_to_movie_lines, path_to_movie_conversations):\n",
        "    id2line = {}\n",
        "    with open(path_to_movie_lines, errors='ignore') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(path_to_movie_conversations, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "            conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "            for i in range(len(conversation) - 1):\n",
        "                inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
        "                outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
        "                if len(inputs) >= MAX_SAMPLES:\n",
        "                    return inputs, outputs\n",
        "    return inputs, outputs\n"
      ],
      "metadata": {
        "id": "-sijVHLArNQJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(path_to_movie_lines,\n",
        "                   path_to_movie_conversations):\n",
        "    questions, answers = load_conversations(path_to_movie_lines, path_to_movie_conversations)\n",
        "\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    counter = Counter()\n",
        "    for sent in questions + answers:\n",
        "        counter.update(tokenizer(sent))\n",
        "\n",
        "    voc = vocab(counter)\n",
        "    voc.insert_token(token=UNK_TOKEN, index=UNK_TOKEN_IND)\n",
        "    voc.set_default_index(index=UNK_TOKEN_IND)\n",
        "    voc.insert_token(token=PAD_TOKEN, index=PAD_TOKEN_IND)\n",
        "    voc.insert_token(token=BOS_TOKEN, index=BOS_TOKEN_IND)\n",
        "    voc.insert_token(token=EOS_TOKEN, index=EOS_TOKEN_IND)\n",
        "\n",
        "    q_tokenized = [text_transform(t, voc, tokenizer) for t in questions]\n",
        "    a_tokenized = [text_transform(t, voc, tokenizer) for t in answers]\n",
        "\n",
        "    import tensorflow as tf # todo\n",
        "    q_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        q_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    a_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        a_tokenized, maxlen=MAX_LENGTH, padding='post', value=1.0)\n",
        "\n",
        "    print(\"Vocab len\", len(voc))\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        list(\n",
        "            zip(\n",
        "                  q_padded.astype(np.float32),\n",
        "                  a_padded.astype(np.float32),\n",
        "                )\n",
        "            ),\n",
        "            batch_size=BATCH,\n",
        "            shuffle=False,\n",
        "    )\n",
        "\n",
        "    print(voc)\n",
        "    torch.save(voc, 'vocab')\n",
        "\n",
        "    return dataloader, text_transform, voc\n"
      ],
      "metadata": {
        "id": "dblp_TVLqvav"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e3T8xN1yxmyc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines_path = 'data/movie_lines.txt'\n",
        "conversations_path = 'data/movie_conversations.txt'    \n",
        "\n",
        "dataloader, text_transform, voc = get_dataloader(lines_path,\n",
        "                                                 conversations_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42tdLcayovn",
        "outputId": "35c850dc-e53a-45c0-8557-5515ef958f3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab len 23068\n",
            "Vocab()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = None\n",
        "\n",
        "for i,x in enumerate(dataloader):\n",
        "  if i > 1: break\n",
        "  print(x[0].shape)\n",
        "  test_sample = x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMyWFI7N3VVf",
        "outputId": "ddaf8ff5-2256-4e4d-9eb4-d9c1ca3402ee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 40])\n",
            "torch.Size([3, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OG_PUCipfDAT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fneural-networks-and%2F9781492037354%2Fassets%2Fmlst_1413.png&f=1&nofb=1\n",
        "class myLSTM(nn.Module):\n",
        "  def __init__(self, isize, osize):\n",
        "    super(myLSTM, self).__init__()\n",
        "\n",
        "    self.isize = isize\n",
        "    self.osize = osize\n",
        "\n",
        "    self.f = nn.Linear(isize, osize)\n",
        "    self.g = nn.Linear(isize, osize)\n",
        "    self.i = nn.Linear(isize, osize)\n",
        "    self.o = nn.Linear(isize, osize)\n",
        "\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forget_gate(self, h, x, c):\n",
        "    fho = self.f(h)\n",
        "    fxo = self.f(x)\n",
        "\n",
        "    fo = fho + fxo\n",
        "    fos = self.sigmoid(fo)\n",
        "    out = torch.mul(c, fos)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def input_gate(self, h, x):\n",
        "    gho = self.g(h)\n",
        "    gxo = self.g(x)\n",
        "    go = gho + gxo\n",
        "    gto = self.tanh(go)\n",
        "\n",
        "    iho = self.i(h)\n",
        "    ixo = self.i(x)\n",
        "    io = iho + ixo\n",
        "    ito = self.sigmoid(io)\n",
        "\n",
        "    out = torch.mul(gto, ito)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def output_gate(self, h, x, cn):\n",
        "    oho = self.o(h)\n",
        "    oxo = self.o(x)\n",
        "    oo = oho + oxo\n",
        "\n",
        "    out = torch.mul(oo, cn)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def forward(self, x, h0=None, c0=None):\n",
        "    if not h0:\n",
        "      h0 = torch.zeros(x.shape[0], self.isize)\n",
        "    if not c0:\n",
        "      c0 = torch.zeros(x.shape[0], self.osize)\n",
        "\n",
        "    fg_out = self.forget_gate(h0, x, c0)\n",
        "    ig_out = self.input_gate(h0, x)\n",
        "\n",
        "    c_new = fg_out + ig_out\n",
        "\n",
        "    cnt = self.tanh(c_new) \n",
        "\n",
        "    h_new = self.output_gate(h0, x, c_new)\n",
        "\n",
        "    out = h_new\n",
        "\n",
        "    return out, (h_new[len(h_new) - 1].reshape(1, self.osize),\n",
        "                 c_new[len(h_new) - 1].reshape(1, self.osize))\n"
      ],
      "metadata": {
        "id": "Iajk3iU8hKlB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_lstm = myLSTM(40, 80)\n",
        "\n",
        "for p in my_lstm.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "out, (h, c) = my_lstm(test_sample)\n",
        "\n",
        "print(out.shape)\n",
        "print('\\n----------------')\n",
        "print(h.shape)\n",
        "print('\\n----------------')\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic6e2QJcHlqj",
        "outputId": "3e1cc135-24f0-46ee-a749-6935e6c404b2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "torch.Size([80, 40])\n",
            "torch.Size([80])\n",
            "\n",
            "\n",
            "torch.Size([3, 80])\n",
            "\n",
            "----------------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "----------------\n",
            "torch.Size([1, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YImpb8ymHlsy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xcvxNbhgfh3S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sample.shape)\n",
        "print(test_sample.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_FMbHcG3-e-",
        "outputId": "eb5dd82d-37db-4330-9368-cd593f9b30c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 40])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_lstm = nn.LSTM(40, 80, batch_first=True)\n",
        "\n",
        "for p in t_lstm.parameters():\n",
        "  print(p.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "res, (h_s, c_s) = t_lstm(test_sample)\n",
        "\n",
        "print(res.shape)\n",
        "print('\\n---------')\n",
        "print(h_s.shape)\n",
        "print('\\n---------')\n",
        "print(c_s.shape)\n",
        "print('\\n---------')"
      ],
      "metadata": {
        "id": "QfkMe8hL0oi7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b38d179-d369-4f1f-b366-5fc3c3e8f3c8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([320, 40])\n",
            "torch.Size([320, 80])\n",
            "torch.Size([320])\n",
            "torch.Size([320])\n",
            "\n",
            "\n",
            "torch.Size([3, 80])\n",
            "\n",
            "---------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "---------\n",
            "torch.Size([1, 80])\n",
            "\n",
            "---------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jxr4MUuSKxHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B_OfGA35gbyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wi7GRvfpevfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QtVP3qoOKxRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y4GSVA0xKxVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s1cpwVbSKxZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Up2jkK36zGAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xrr5i0IpzGCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LGWG3mbzzGEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mum903TXzGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LfUhnHL_Kxcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ddwvBDsnKxf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/movie_conversations.txt | tail -n 10"
      ],
      "metadata": {
        "id": "4kVqD08vKxjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "zKWhBqLmKxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/README.txt"
      ],
      "metadata": {
        "id": "mm4in0NdubQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # trash\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def preprocess(x):\n",
        "#   x_no_new = x.replace('\\n', '')\n",
        "#   text = x_no_new.split(FIELD_SPLITTER).pop()\n",
        "#   embedding = g_vectors.get_vecs_by_tokens(tokenizer(text), lower_case_backup=True)\n",
        "#   return embedding\n",
        "\n",
        "# tokenizer = get_tokenizer('basic_english')\n",
        "# g_vectors = GloVe(name='840B')\n",
        "# g_vocab = vocab(g_vectors.stoi)\n",
        "\n",
        "\n",
        "# train_iter = tt.data.BucketIterator(\n",
        "#   dataset=train_obj,\n",
        "#   batch_size = 2,\n",
        "#   sort_key=lambda x: len(x.review),\n",
        "#   shuffle=True,\n",
        "#   device=DEVICE\n",
        "# )\n",
        "\n",
        "# trainloader = torch.utils.data.DataLoader(\n",
        "# \t,\n",
        "# \tbatch_size=BATCH,\n",
        "# \tnum_workers=12,\n",
        "# \tshuffle=True\n",
        "# )"
      ],
      "metadata": {
        "id": "UBDoHXDJsqu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"),\n",
        "#                                                lower_case_backup=True)\n",
        "# embeddings\n",
        "# \n",
        "# \n",
        "# \n",
        "# def batch(iterable, size):\n",
        "#     from itertools import chain, islice\n",
        "#     iterator = iter(iterable)\n",
        "#     for first in iterator:\n",
        "#         yield list(chain([first], islice(iterator, size - 1)))"
      ],
      "metadata": {
        "id": "DRxfxpjQsrTJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}